# -*- coding: utf-8 -*-
"""AtomicHabitsNLP.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WUV4a0EVtNYvyJz258Q9j7nmQ3oNow7I
"""

!pip install numpy==1.26.4 pandas==2.2.2 seaborn matplotlib==3.10.1 --quiet

# Atomic Habits NLP Analysis

# Step 1: Install Required Libraries
!pip install PyMuPDF spacy nltk wordcloud textblob gensim seaborn transformers --quiet
!python -m textblob.download_corpora

# Step 2: Imports and Downloads
import fitz  # PyMuPDF
import nltk
import re
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from wordcloud import WordCloud
from textblob import TextBlob
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
from transformers import pipeline
from IPython.display import Markdown, display

nltk.download('punkt')
nltk.download('stopwords')

# Step 3: Load PDF
doc = fitz.open("/content/Atomic habits.pdf")
full_text = ""
for page in doc:
    full_text += page.get_text()

# Step 4: Sentiment Analysis
blob = TextBlob(full_text)
print(f"Sentiment polarity: {blob.sentiment.polarity}, Subjectivity: {blob.sentiment.subjectivity}")

# Step 5: Clean and Tokenize Text
tokens = word_tokenize(re.sub(r'[^a-zA-Z ]', '', full_text.lower()))
tokens = [w for w in tokens if w not in stopwords.words('english') and len(w) > 3]

# Step 6: Word Cloud
wordcloud = WordCloud(width=800, height=400).generate(' '.join(tokens))
plt.figure(figsize=(10, 5))
plt.imshow(wordcloud, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Atomic Habits')
plt.show()

# Step 7: TF-IDF Analysis
from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(stop_words='english', max_features=10)
X = tfidf.fit_transform([full_text])
scores = zip(tfidf.get_feature_names_out(), X.toarray()[0])
df_keywords = pd.DataFrame(scores, columns=['Keyword', 'Score']).sort_values(by='Score', ascending=True)
df_keywords.plot.barh(x='Keyword', y='Score', legend=False, color='green')
plt.title('Top TF-IDF Keywords')
plt.xlabel('Importance')
plt.tight_layout()
plt.show()

# Step 8: Quote Extraction
quotes = re.findall(r'‚Äú([^‚Äù]{20,})‚Äù', full_text)
for i, quote in enumerate(quotes[:5]):
    display(Markdown(f"> üí¨ **Quote {i+1}:** *{quote.strip()}*"))

# Step 9: Heatmap of Keyword Frequency Across Chapters (Simulated)
chapters = ["Ch 1", "Ch 2", "Ch 3", "Ch 4", "Ch 5"]
keywords = ["habit", "identity", "cue", "reward", "environment"]
matrix = np.random.randint(0, 10, size=(len(chapters), len(keywords)))
df_matrix = pd.DataFrame(matrix, index=chapters, columns=keywords)
plt.figure(figsize=(8, 6))
sns.heatmap(df_matrix, annot=True, cmap="YlGnBu", fmt="d")
plt.title("Keyword Frequency Across Chapters")
plt.ylabel("Chapters")
plt.xlabel("Keywords")
plt.show()

# Step 10: Summarization using Transformers
summarizer = pipeline("summarization", model="facebook/bart-large-cnn")
chunk = full_text[3000:5000]
summary = summarizer(chunk, max_length=130, min_length=30, do_sample=False)
print("üìù Summary:\n", summary[0]['summary_text'])

start_index = full_text.find("Introduction")
chunk = full_text[start_index:start_index + 1500]
summary = summarizer(chunk, max_length=150, min_length=50, do_sample=False)
print("üìò Introduction Summary:\n", summary[0]['summary_text'])

"""### Chapter wise insights Table"""

import re

# Step 1: Extract full text from all pages
full_text = ""
for page in doc:
    full_text += page.get_text()

# Step 2: Normalize the text
clean_text = full_text.replace("\t", " ")               # convert tabs to spaces
clean_text = re.sub(r" {2,}", " ", clean_text)          # collapse multiple spaces
clean_text = re.sub(r"\n{2,}", "\n\n", clean_text)      # normalize blank lines

# Step 3: Find lines that contain ONLY a chapter number (1‚Äì20)
chapter_number_lines = list(re.finditer(r"(?m)^\s*(?:[1-9]|1\d|20)\s*$", clean_text))
chapter_number_lines = chapter_number_lines[:20]

# Confirm we found all 20 chapters
if len(chapter_number_lines) != 20:
    print(f"‚ö†Ô∏è Expected 20 chapters, found {len(chapter_number_lines)}")
else:
    print("‚úÖ All 20 chapters detected")

# Step 4: Slice text into chapter chunks
chapter_chunks = []
for i in range(len(chapter_number_lines)):
    start = chapter_number_lines[i].start()
    end = chapter_number_lines[i + 1].start() if i + 1 < len(chapter_number_lines) else len(clean_text)
    content = clean_text[start:end].strip()
    chapter_chunks.append(content)

# Step 5: Preview each chapter length
for i, chapter in enumerate(chapter_chunks):
    print(f"üìò Chapter {i + 1} ‚Äî {len(chapter)} characters")

# You now have a list `chapter_chunks` containing all chapter texts

import re

# Step 1: Extract full text from all pages
full_text = ""
for page in doc:
    full_text += page.get_text()

# Step 2: Normalize the text
clean_text = full_text.replace("\t", " ")               # convert tabs to spaces
clean_text = re.sub(r" {2,}", " ", clean_text)          # collapse multiple spaces
clean_text = re.sub(r"\n{2,}", "\n\n", clean_text)      # normalize blank lines

# Step 3: Find lines that contain ONLY a chapter number (1‚Äì20)
chapter_number_lines = list(re.finditer(r"(?m)^\s*(?:[1-9]|1\d|20)\s*$", clean_text))

if len(chapter_number_lines) != 20:
    print(f"‚ö†Ô∏è Warning: Found {len(chapter_number_lines)} chapter-like matches. Trimming to 20.")
    chapter_number_lines = chapter_number_lines[:20]
else:
    print("‚úÖ All 20 chapters detected")


# Step 4: Slice text into chapter chunks
chapter_chunks = []
for i in range(len(chapter_number_lines)):
    start = chapter_number_lines[i].start()
    end = chapter_number_lines[i + 1].start() if i + 1 < len(chapter_number_lines) else len(clean_text)
    content = clean_text[start:end].strip()
    chapter_chunks.append(content)

# Step 5: Preview each chapter length
for i, chapter in enumerate(chapter_chunks):
    print(f"üìò Chapter {i + 1} ‚Äî {len(chapter)} characters")

#!pip install transformers

from transformers import pipeline

# Load the summarizer model
summarizer = pipeline("summarization", model="sshleifer/distilbart-cnn-12-6")

# Summarize each chapter (limit tokens to avoid overflow)
chapter_summaries = []
for i, (title, text) in enumerate(zip(range(1, 21), chapter_chunks)):
    print(f"‚úÇÔ∏è Summarizing Chapter {title}...")

    # Optional: truncate to ~1024 tokens (Bart's max input)
    input_text = text[:3000]

    summary = summarizer(input_text, max_length=150, min_length=50, do_sample=False)[0]['summary_text']
    chapter_summaries.append(summary)

from textblob import TextBlob

chapter_sentiments = []
for chapter in chapter_chunks:
    blob = TextBlob(chapter)
    chapter_sentiments.append(blob.sentiment.polarity)

from sklearn.feature_extraction.text import TfidfVectorizer

top_keywords = []
vectorizer = TfidfVectorizer(stop_words="english", max_features=20)
for chapter in chapter_chunks:
    tfidf = vectorizer.fit_transform([chapter])
    scores = zip(vectorizer.get_feature_names_out(), tfidf.toarray()[0])
    keywords = sorted(scores, key=lambda x: x[1], reverse=True)[:5]
    top_keywords.append([k[0] for k in keywords])

import pandas as pd

df = pd.DataFrame({
    "Chapter": list(range(1, 21)),
    "Summary": chapter_summaries,
    "Sentiment": chapter_sentiments,
    "Top Keywords": top_keywords
})

# Preview
df.head()

#save data to csv
df.to_csv('atomic_habits_analysis.csv', index=False)

"""### CONCEPT NETWORK"""

import networkx as nx
import matplotlib.pyplot as plt
from collections import defaultdict
import re

# Step 1: Define core concepts
concepts = ["cue", "craving", "response", "reward", "identity", "environment", "routine", "system", "habit", "behavior"]

# Step 2: Count co-occurrences across chapters
co_occurrence = defaultdict(int)

for chapter in chapter_chunks:
    words = set(re.findall(r"\b\w+\b", chapter.lower()))
    present = [c for c in concepts if c in words]

    for i in range(len(present)):
        for j in range(i+1, len(present)):
            pair = tuple(sorted((present[i], present[j])))
            co_occurrence[pair] += 1

# Step 3: Build the network
G = nx.Graph()
for concept in concepts:
    G.add_node(concept)

for (a, b), weight in co_occurrence.items():
    if weight > 0:
        G.add_edge(a, b, weight=weight)

# Step 4: Visualize
plt.figure(figsize=(10, 8))
pos = nx.spring_layout(G, k=0.5)
edges = G.edges()
weights = [G[u][v]['weight'] for u, v in edges]

nx.draw(G, pos, with_labels=True, node_color="skyblue", node_size=1500, font_size=12, width=weights)
plt.title("üîó Concept Relationship Network in Atomic Habits")
plt.show()

!pip install pyvis

from pyvis.network import Network
import networkx as nx
from collections import defaultdict
import re
from google.colab import files

# Step 1: Define your concepts
concepts = ["cue", "craving", "response", "reward", "identity", "environment", "routine", "system", "habit", "behavior"]

# Step 2: Count co-occurrence
co_occurrence = defaultdict(int)
for chapter in chapter_chunks:
    words = set(re.findall(r"\b\w+\b", chapter.lower()))
    present = [c for c in concepts if c in words]
    for i in range(len(present)):
        for j in range(i + 1, len(present)):
            pair = tuple(sorted((present[i], present[j])))
            co_occurrence[pair] += 1

# Step 3: Build NetworkX graph with weights
G = nx.Graph()
for concept in concepts:
    G.add_node(concept)

for (a, b), weight in co_occurrence.items():
    if weight > 0:
        G.add_edge(a, b, weight=weight)

# Step 4: Create Pyvis network
net = Network(height="700px", width="100%", bgcolor="#ffffff", font_color="black", notebook=True)

for node in G.nodes:
    net.add_node(node, label=node)

for source, target, data in G.edges(data=True):
    net.add_edge(source, target, value=data.get("weight", 1))  # edge thickness

# Step 5: Save and show
net.show("atomic_habits_concept_network.html")
files.download("atomic_habits_concept_network.html")